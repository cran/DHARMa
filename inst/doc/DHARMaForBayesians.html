<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Florian Hartig, Theoretical Ecology, University of Regensburg website" />

<meta name="date" content="2022-09-08" />

<title>DHARMa for Bayesians</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>

<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">DHARMa for Bayesians</h1>
<h4 class="author">Florian Hartig, Theoretical Ecology, University of
Regensburg <a href="https://www.uni-regensburg.de/biologie-vorklinische-medizin/theoretische-oekologie/mitarbeiter/hartig/">website</a></h4>
<h4 class="date">2022-09-08</h4>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>The ‘DHARMa’ package uses a simulation-based approach to create
readily interpretable scaled (quantile) residuals for fitted
(generalized) linear mixed models. This Vignette describes how to user
DHARMa vor checking Bayesian models. It is recommended to read this
AFTER the general DHARMa vignette, as all comments made there (in
pacticular regarding the interpretation of the residuals) also apply to
Bayesian models.</p>
</div>


<div id="TOC">
<ul>
<li><a href="#basic-workflow" id="toc-basic-workflow">Basic workflow</a>
<ul>
<li><a href="#example-in-jags" id="toc-example-in-jags">Example in
Jags</a></li>
<li><a href="#exercise" id="toc-exercise">Exercise</a></li>
</ul></li>
<li><a href="#conditional-vs.-unconditional-simulations-in-hierarchical-models" id="toc-conditional-vs.-unconditional-simulations-in-hierarchical-models">Conditional
vs. unconditional simulations in hierarchical models</a></li>
<li><a href="#statistical-differences-between-bayesian-vs.-mle-quantile-residuals" id="toc-statistical-differences-between-bayesian-vs.-mle-quantile-residuals">Statistical
differences between Bayesian vs. MLE quantile residuals</a></li>
</ul>
</div>

<div id="basic-workflow" class="section level1">
<h1>Basic workflow</h1>
<p>In principle, DHARMa residuals can be calculated and interpreted for
Bayesian models in very much the same way as for frequentist models.
Therefore, all comments regarding tests, residual interpretation etc.
from the main vignette are equally valid for Bayesian model checks.
There are some minor differences regarding the expected null
distribution of the residuals, in particular in the low data limit, but
I believe that for most people, these are of less concern.</p>
<p>The main difference for a Bayesian user is that, unlike for users of
directly supported regression packages such as lme4 or glmmTMB, most
Bayesian users will have to create the simulations for the fitted model
themselves and then feed them into DHARMa by hand. The basic workflow
for Bayesians that work with BUGS, JAGS, STAN or similar is:</p>
<ol style="list-style-type: decimal">
<li>Create posterior predictive simulations for your model</li>
<li>Read these in with the createDHARMa function</li>
<li>Interpret those as described in the main vignette</li>
</ol>
<p>This is more easy as it sounds. For the major Bayesian samplers
(e.g. BUGS, JAGS, STAN), it amounts to adding a block with data
simulations to the model, and observing those during the MCMC sampling.
Then, feed the simulations into DHARMa via createDHARMa, and all else
will work pretty much the same as in the main vignette.</p>
<div id="example-in-jags" class="section level2">
<h2>Example in Jags</h2>
<p>Here is an example, with JAGS</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rjags)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BayesianTools)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> DHARMa<span class="sc">::</span><span class="fu">createData</span>(<span class="dv">200</span>, <span class="at">overdispersion =</span> <span class="fl">0.2</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>Data <span class="ot">=</span> <span class="fu">as.list</span>(dat)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>Data<span class="sc">$</span>nobs <span class="ot">=</span> <span class="fu">nrow</span>(dat)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>Data<span class="sc">$</span>nGroups <span class="ot">=</span> <span class="fu">length</span>(<span class="fu">unique</span>(dat<span class="sc">$</span>group))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>modelCode <span class="ot">=</span> <span class="st">&quot;model{</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="st">  for(i in 1:nobs){</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="st">    observedResponse[i] ~ dpois(lambda[i])  # poisson error distribution</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="st">    lambda[i] &lt;- exp(eta[i]) # inverse link function</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="st">    eta[i] &lt;- intercept + env*Environment1[i]  # linear predictor</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="st">  intercept ~ dnorm(0,0.0001)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="st">  env ~ dnorm(0,0.0001)</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="st">  # Posterior predictive simulations </span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="st">  for (i in 1:nobs) {</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="st">    observedResponseSim[i]~dpois(lambda[i])</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="st">}&quot;</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>jagsModel <span class="ot">&lt;-</span> <span class="fu">jags.model</span>(<span class="at">file=</span> <span class="fu">textConnection</span>(modelCode), <span class="at">data=</span>Data, <span class="at">n.chains =</span> <span class="dv">3</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>para.names <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;intercept&quot;</span>,<span class="st">&quot;env&quot;</span>, <span class="st">&quot;lambda&quot;</span>, <span class="st">&quot;observedResponseSim&quot;</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>Samples <span class="ot">&lt;-</span> <span class="fu">coda.samples</span>(jagsModel, <span class="at">variable.names =</span> para.names, <span class="at">n.iter =</span> <span class="dv">5000</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> BayesianTools<span class="sc">::</span><span class="fu">getSample</span>(Samples)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(x) <span class="co"># problem: all the variables are in one array - this is better in STAN, where this is a list - have to extract the right columns by hand</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>posteriorPredDistr <span class="ot">=</span> x[,<span class="dv">3</span><span class="sc">:</span><span class="dv">202</span>] <span class="co"># this is the uncertainty of the mean prediction (lambda)</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>posteriorPredSim <span class="ot">=</span> x[,<span class="dv">203</span><span class="sc">:</span><span class="dv">402</span>] <span class="co"># these are the simulations </span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>sim <span class="ot">=</span> <span class="fu">createDHARMa</span>(<span class="at">simulatedResponse =</span> <span class="fu">t</span>(posteriorPredSim), <span class="at">observedResponse =</span> dat<span class="sc">$</span>observedResponse, <span class="at">fittedPredictedResponse =</span> <span class="fu">apply</span>(posteriorPredDistr, <span class="dv">2</span>, median), <span class="at">integerResponse =</span> T)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(sim)</span></code></pre></div>
<p>In the created plots, you will see overdispersion, which is
completely expected, as the simulated data has overdispersion and a RE,
which is not accounted for by the Jags model.</p>
</div>
<div id="exercise" class="section level2">
<h2>Exercise</h2>
<p>As an exercise, you could now:</p>
<ul>
<li>Add a RE</li>
<li>Account for overdispersion, e.g. via an OLRE or a negative
Binomial</li>
</ul>
<p>And check how the residuals improve.</p>
</div>
</div>
<div id="conditional-vs.-unconditional-simulations-in-hierarchical-models" class="section level1">
<h1>Conditional vs. unconditional simulations in hierarchical
models</h1>
<p>The most important consideration in using DHARMa with Bayesian models
is how to create the simulations. You can see in my jags code that the
block</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Posterior predictive simulations </span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nobs) {</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    observedResponseSim[i]<span class="sc">~</span><span class="fu">dpois</span>(lambda[i])</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p>performs the posterior predictive simulations. Here, we just take the
predicted lambda (mean perdictions) during the MCMC simulations and
sample from the assumed distribution. This will works for any
non-hierarchical model.</p>
<p>When we move to hierarchical or multi-level models, including GLMMs,
the issue of simulation becomes a bit more complicated. In a
hierarchical model, there are several random processes that sit on top
of each other. In the same way as explained in the main vignette at the
point conditional / unconditional simulations, we will have to decide
which of these random processes should be included in the posterior
predictive simulations.</p>
<p>As an example, imagine we add a RE in the likelihood of the previous
model, to account for the group structure in the data.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nobs){</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    observedResponse[i] <span class="sc">~</span> <span class="fu">dpois</span>(lambda[i])  <span class="co"># poisson error distribution</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    lambda[i] <span class="ot">&lt;-</span> <span class="fu">exp</span>(eta[i]) <span class="co"># inverse link function</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    eta[i] <span class="ot">&lt;-</span> intercept <span class="sc">+</span> env<span class="sc">*</span>Environment1[i] <span class="sc">+</span> RE[group[i]] <span class="co"># linear predictor</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nGroups){</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>   RE[j] <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>,tauRE)  </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p>The predictions lambda[i] now depend on a lower-level stochastic
effect, which is described by RE[j] ~ dnorm(0,tauRE). We can now decide
to create posterior predictive simulations conditional on posterior
estimates RE[j] (conditional simulations), in which case we would have
to change nothing in the block for the posterior predictive simulations.
Alternatively, we can decide that we want to re-simulate the RE
(unconditional simulations), in which case we have to copy the entire
structure of the likelihood in the predictions</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nGroups){</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>   RESim[j] <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>,tauRE)  </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nobs) {</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    observedResponseSim[i] <span class="sc">~</span> <span class="fu">dpois</span>(lambdaSim[i]) </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    lambdaSim[i] <span class="ot">&lt;-</span> <span class="fu">exp</span>(etaSim[i]) </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    etaSim[i] <span class="ot">&lt;-</span> intercept <span class="sc">+</span> env<span class="sc">*</span>Environment1[i] <span class="sc">+</span> RESim[group[i]] </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p>Essentially, you can remember that if you want full (uncoditional)
simulations, you basically have to copy the entire likelihood of the
hierarchical model, minus the priors, and sample along the hierarchical
model structure. If you want to condition on a part of this structure,
just cut the DAG at the point on which you want to condition on.</p>
</div>
<div id="statistical-differences-between-bayesian-vs.-mle-quantile-residuals" class="section level1">
<h1>Statistical differences between Bayesian vs. MLE quantile
residuals</h1>
<p>A common question is if there are differences between Bayesian and
MLE quantile residuals.</p>
<p>First of all, note that MLE and Bayesian quantile residuals are not
exactly identical. The main difference is in how the simulation of the
data under the fitted model are performed:</p>
<ul>
<li><p>For models fitted by MLE, simulations in DHARMa are with the MLE
(point estimate)</p></li>
<li><p>For models fitted with Bayes, simulations are practically always
performed while also drawing from the posterior parameter uncertainty
(as a point estimate is not available).</p></li>
</ul>
<p>Thus, Bayesian posterior predictive simulations include the
parametric uncertainty of the model, additionally to the sampling
uncertainty. From this we can directly conclude that Bayesian and MLE
quantile residuals are asymptotically identical (and via the usual
arguments uniformly distributed), but become more different the smaller
n becomes.</p>
<p>To examine what those differences are, let’s imagine that we start
with a situation of infinite data. In this case, we have a “sharp”
posterior that can be viewed as identical to the MLE.</p>
<p>If we reduce the number of data, there are two things happening</p>
<ol style="list-style-type: decimal">
<li><p>The posterior gets wider, with the likelihood component being
normally distributed, at least initially</p></li>
<li><p>The influence of the prior increases, the faster the stronger the
prior is.</p></li>
</ol>
<p>Thus, if we reduce the data, for weak / uninformative priors, we will
simulate data while sampling parameters from a normal distribution
around the MLE, while for strong priors, we will effectively sample data
while drawing parameters of the model from the prior.</p>
<p>In particular in the latter case (prior dominates, which can be
checked via prior sensitivity analysis), you may see residual patterns
that are caused by the prior, even though the model structure is
correct. In some sense, you could say that the residuals check if the
combination of prior + structure is compatible with the data. It’s a
philosophical debate how to react on such a deviation, as the prior is
not really negotiable in a Bayesian analysis.</p>
<p>Of course, also the MLE distribution might get problems in low data
situations, but I would argue that MLE is usually only used anyway if
the MLE is reasonably sharp. In practice, I have self experienced
problems with MLE estimates. It’s a bit different in the Bayesian case,
where it is possible and often done to fit very complex models with
limited data. In this case, many of the general issues in defining null
distributions for Bayesian p-values (as, e.g., reviewed in <a href="https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecm.1314">Conn
et al., 2018</a>) apply.</p>
<p>I would add though that while I find it important that users are
aware of those differences, I have found that in practice these issues
are small, and usually overruled by the much stronger effects of model
error.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
